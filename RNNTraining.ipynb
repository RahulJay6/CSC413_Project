{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CBJ-_ys_485w","executionInfo":{"status":"ok","timestamp":1713395425766,"user_tz":240,"elapsed":23688,"user":{"displayName":"Vishnu Akundi","userId":"06434560699444501603"}},"outputId":"7779910c-aef3-442a-a32b-8e4c37033c6e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# Get Training labels\n","import pandas as pd\n","from sklearn.preprocessing import OneHotEncoder\n","import numpy as np\n","df = pd.read_csv('/content/drive/MyDrive/CSC413FinalProject/liar_dataset/train.tsv', delimiter='\\t', header=None)\n","# Optionally, add column names if the file doesn't include headers\n","df.columns = [\"ID\", \"Label\", \"Statement\", \"Subject\", \"Speaker\", \"Speaker_Job\", \"Speaker_State\", \"Party\", \"barely_true_counts\", \"false_counts\", \"half_true_counts\", \"mostly_true_counts\", \"pants_on_fire_counts\", \"context\"]\n","\n","\n","labels = list(df['Label'])\n","\n","# Example list\n","categories = np.array(labels).reshape(-1, 1)\n","\n","# Create the encoder and fit it\n","encoder = OneHotEncoder(sparse=False)\n","labels = encoder.fit_transform(categories)\n","\n","print(labels.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g_ZNsC6a5CcZ","executionInfo":{"status":"ok","timestamp":1713395430162,"user_tz":240,"elapsed":4022,"user":{"displayName":"Vishnu Akundi","userId":"06434560699444501603"}},"outputId":"a4847fb7-7261-481b-8714-3ae84be90b07"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(10240, 6)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# Get embeddings and metadata\n","import torch\n","train_emb = torch.load('/content/drive/MyDrive/CSC413FinalProject/train_embeddings.pt').to('cuda:0')\n","val_emb = torch.load('/content/drive/MyDrive/CSC413FinalProject/val_embeddings.pt').to('cuda:0')\n","\n","train_meta = torch.load('/content/drive/MyDrive/CSC413FinalProject/train_meta.pt').to('cuda:0')\n","val_meta = torch.load('/content/drive/MyDrive/CSC413FinalProject/valid_meta.pt').to('cuda:0')\n","\n","train_input = torch.cat((train_emb,train_meta), dim=1)\n","val_input = torch.cat((val_emb,val_meta), dim=1)\n","\n","train_emb = train_emb.unsqueeze(1)\n","\n","print(train_input.shape)\n","print(val_input.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ntm4C_8L-IVL","executionInfo":{"status":"ok","timestamp":1713395442560,"user_tz":240,"elapsed":12402,"user":{"displayName":"Vishnu Akundi","userId":"06434560699444501603"}},"outputId":"afb4d9e9-78c7-4907-b6b6-b19b800d1e8f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([10240, 938])\n","torch.Size([1284, 938])\n"]}]},{"cell_type":"code","source":["# Get validation Labels\n","df_val = pd.read_csv('/content/drive/MyDrive/CSC413FinalProject/liar_dataset/valid.tsv', delimiter='\\t', header=None)\n","# Optionally, add column names if the file doesn't include headers\n","df_val.columns = [\"ID\", \"Label\", \"Statement\", \"Subject\", \"Speaker\", \"Speaker_Job\", \"Speaker_State\", \"Party\", \"barely_true_counts\", \"false_counts\", \"half_true_counts\", \"mostly_true_counts\", \"pants_on_fire_counts\", \"context\"]\n","\n","val_labels = list(df_val['Label'])\n","\n","# Example list\n","categories = np.array(val_labels).reshape(-1, 1)\n","\n","# Create the encoder and fit it\n","encoder = OneHotEncoder(sparse=False)\n","val_labels = encoder.fit_transform(categories)\n","\n","print(val_labels.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_sofIQ0S-NID","executionInfo":{"status":"ok","timestamp":1713395442898,"user_tz":240,"elapsed":343,"user":{"displayName":"Vishnu Akundi","userId":"06434560699444501603"}},"outputId":"5f4136c5-2e45-4de5-8cd7-4b7744913d91"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(1284, 6)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["#Remove nan from input\n","# Check for NaN values\n","nan_mask = torch.isnan(train_input)\n","nan_mask_2 = torch.isnan(val_input)\n","\n","# Replace NaN values with zeros\n","train_input[nan_mask] = 0\n","val_input[nan_mask_2] = 0"],"metadata":{"id":"HTB_G_IL-RgT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","\n","\n","# Define RNN model\n","class RNNModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size):\n","        super(RNNModel, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x):\n","        # Initialize hidden state with zeros\n","        batch_size = x.size(0)  # Get batch size\n","        h0 = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n","\n","        # Forward propagate RNN\n","        out, _ = self.rnn(x, h0)\n","\n","        # Decode the hidden state of the last time step\n","        out = self.fc(out[:, -1, :])\n","        return out\n","\n","# Parameters\n","input_size = 768  # Size of BERT embeddings\n","hidden_size = 128  # Hidden size of RNN\n","output_size = 6  # Size of output labels\n","learning_rate = 0.001\n","num_epochs = 10\n","batch_size = 64\n","\n","# Define RNN model\n","rnnmodel = RNNModel(input_size, hidden_size, output_size)\n","\n","# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(rnnmodel.parameters(), lr=learning_rate)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    for i in range(0, len(train_emb), batch_size):\n","        # Get mini-batch data\n","        inputs = train_emb[i:i+batch_size].to('cpu')\n","        print(inputs.shape)\n","        targets = labels[i:i+batch_size]\n","\n","        # Forward pass\n","        outputs = rnnmodel(inputs)\n","\n","        # Compute loss\n","        loss = criterion(outputs, torch.tensor(targets))\n","\n","        # Backward pass and optimization\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if (i == 0):\n","            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n","                  .format(epoch+1, num_epochs, i+1, len(train_emb)//batch_size, loss.item()))\n"],"metadata":{"id":"OPUWq_wWHW4j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# BELOW IS TRAINING WITH RNN OUTPUTS + METADATA:\n","\n","train_full_input = torch.cat((model(train_emb.to('cpu')),train_meta.to('cpu')), dim=1).detach()\n","print(train_full_input.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZoMnp91BKlsW","executionInfo":{"status":"ok","timestamp":1713310400833,"user_tz":240,"elapsed":271,"user":{"displayName":"Rahul Hingorani","userId":"08222434025225003866"}},"outputId":"a4638623-e296-408e-b310-54b47382aea3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([10240, 176])\n"]}]},{"cell_type":"code","source":["#Remove nan from input\n","# Check for NaN values\n","nan_mask = torch.isnan(train_full_input)\n","\n","# Replace NaN values with zeros\n","train_full_input[nan_mask] = 0"],"metadata":{"id":"wzglJWTNLOPI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define model\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","class LogisticRegression(nn.Module):\n","    def __init__(self, input_dim, num_classes, hidden = 500):\n","        super(LogisticRegression, self).__init__()\n","        self.linear = nn.Linear(input_dim, hidden)\n","        self.hidden_linear = nn.Linear(hidden, num_classes)\n","\n","    def forward(self, x):\n","        x = self.linear(x)  # Output logits for each class\n","        x = nn.functional.relu(x)\n","        x = self.hidden_linear(x)\n","        return nn.functional.softmax(x, dim=1)"],"metadata":{"id":"jSaLM4k-Lgug"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def training(train_embeddings, labels, model, epochs=10):\n","  dataset = TensorDataset(train_embeddings, torch.Tensor(labels).float())\n","  data_loader = DataLoader(dataset, batch_size=10, shuffle=True)\n","\n","\n","\n","  model = model\n","\n","  # Loss and optimizer\n","  criterion = nn.BCELoss()\n","  optimizer = optim.SGD(model.parameters(), lr=0.001)\n","  num_epochs = epochs  # Set the number of epochs\n","\n","  for epoch in range(num_epochs):\n","      for inputs, targets in data_loader:\n","          # Forward pass\n","          outputs = model(inputs.to(device='cpu'))\n","          loss = criterion(outputs, targets)\n","\n","          # Backward and optimize\n","          optimizer.zero_grad()\n","          loss.backward()\n","          optimizer.step()\n","\n","      print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n","  return model"],"metadata":{"id":"fHNz0uxULi8Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_full_input = train_full_input.to(torch.float)\n","training_model = LogisticRegression(train_full_input.shape[1], 6)\n","training_model = training(train_full_input, labels, training_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wjxG1cjnLqF_","executionInfo":{"status":"ok","timestamp":1713309422405,"user_tz":240,"elapsed":12521,"user":{"displayName":"Rahul Hingorani","userId":"08222434025225003866"}},"outputId":"bb53ad41-0266-4af8-e914-661ba5c87a1a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/10], Loss: 0.4579\n","Epoch [2/10], Loss: 0.4363\n","Epoch [3/10], Loss: 0.4983\n","Epoch [4/10], Loss: 0.4626\n","Epoch [5/10], Loss: 0.4170\n","Epoch [6/10], Loss: 0.4471\n","Epoch [7/10], Loss: 0.4560\n","Epoch [8/10], Loss: 0.4277\n","Epoch [9/10], Loss: 0.4565\n","Epoch [10/10], Loss: 0.4196\n"]}]},{"cell_type":"code","source":["#Train Accuracy\n","model_outputs = torch.argmax(training_model(train_full_input.to('cpu')), axis =1)\n","argmax_labels = torch.argmax(torch.Tensor(labels), axis =1)\n","print(sum(model_outputs == argmax_labels)/len(model_outputs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DbRgd3DfLxCr","executionInfo":{"status":"ok","timestamp":1713309451959,"user_tz":240,"elapsed":230,"user":{"displayName":"Rahul Hingorani","userId":"08222434025225003866"}},"outputId":"66006171-77b2-46da-d361-0b35a3fd0844"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.3757)\n"]}]},{"cell_type":"code","source":["# Accuracy of just the RNN\n","model_outputs = torch.argmax(rnnmodel(train_emb.to('cpu')), axis =1)\n","argmax_labels = torch.argmax(torch.Tensor(labels), axis =1)\n","print(sum(model_outputs == argmax_labels)/len(model_outputs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jj7zkF0ON5XW","executionInfo":{"status":"ok","timestamp":1713309807065,"user_tz":240,"elapsed":492,"user":{"displayName":"Rahul Hingorani","userId":"08222434025225003866"}},"outputId":"c3b7ebbe-7db7-42ce-89c3-55442a5c7094"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.3890)\n"]}]}]}